BDEPEND=blas? ( sci-libs/openblas:= ) cublas? ( dev-util/nvidia-cuda-toolkit ) >=dev-util/ninja-1.8.2 >=dev-util/cmake-3.20.5
DEFINED_PHASES=compile configure install postinst prepare test
DEPEND=blas? ( sci-libs/openblas:= ) cublas? ( dev-util/nvidia-cuda-toolkit )
DESCRIPTION=Port of Facebook's LLaMA model in C/C++
EAPI=8
HOMEPAGE=https://github.com/ggerganov/llama.cpp
INHERIT=cmake
IUSE=blas cublas lto tests tools
KEYWORDS=~amd64
LICENSE=MIT
RDEPEND=blas? ( sci-libs/openblas:= ) cublas? ( dev-util/nvidia-cuda-toolkit )
SLOT=0
SRC_URI=https://github.com/ggerganov/llama.cpp/archive/refs/tags/b1331.tar.gz -> llama.cpp-b1331.tar.gz
_eclasses_=toolchain-funcs	eed10cf5e5a06916e654d31f5a1925cc	multilib	c19072c3cd7ac5cb21de013f7e9832e0	flag-o-matic	b1206d933fc5a96cc864180a3cdbf18c	multiprocessing	30ead54fa2e2b5f9cd4e612ffc34d0fe	ninja-utils	39e7a84b06eff4efd9f2e0c3d1668b98	xdg-utils	baea6080dd821f5562d715887954c9d3	cmake	0f2e0c197fad0312f3c4765c9cf36271
_md5_=faab8057ce71b10683410bd49ce10f70
