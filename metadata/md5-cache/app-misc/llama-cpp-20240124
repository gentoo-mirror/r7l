BDEPEND=blas? ( sci-libs/openblas:= ) cublas? ( dev-util/nvidia-cuda-toolkit ) rocm? ( sci-libs/rocBLAS ) app-alternatives/ninja >=dev-build/cmake-3.20.5
DEFINED_PHASES=compile configure install postinst prepare setup test
DEPEND=blas? ( sci-libs/openblas:= ) cublas? ( dev-util/nvidia-cuda-toolkit ) rocm? ( sci-libs/rocBLAS ) !!sys-devel/llvm:0
DESCRIPTION=Port of Facebook's LLaMA model in C/C++
EAPI=8
HOMEPAGE=https://github.com/ggerganov/llama.cpp
INHERIT=cmake llvm rocm
IUSE=blas cublas lto tests tools rocm +amdgpu_targets_gfx906 +amdgpu_targets_gfx908 +amdgpu_targets_gfx90a +amdgpu_targets_gfx1030 amdgpu_targets_gfx803 amdgpu_targets_gfx900 amdgpu_targets_gfx1010 amdgpu_targets_gfx1011 amdgpu_targets_gfx1012 amdgpu_targets_gfx1031 amdgpu_targets_gfx1100 amdgpu_targets_gfx1101 amdgpu_targets_gfx1102
KEYWORDS=~amd64
LICENSE=MIT
RDEPEND=blas? ( sci-libs/openblas:= ) cublas? ( dev-util/nvidia-cuda-toolkit ) rocm? ( sci-libs/rocBLAS )
SLOT=0
SRC_URI=https://github.com/ggerganov/llama.cpp/archive/refs/tags/b1960.tar.gz -> llama.cpp-b1960.tar.gz
_eclasses_=toolchain-funcs	e56c7649b804f051623c8bc1a1c44084	multilib	c19072c3cd7ac5cb21de013f7e9832e0	flag-o-matic	78cf3cc2d5572fddf5e5e10c70f7c81a	multiprocessing	30ead54fa2e2b5f9cd4e612ffc34d0fe	ninja-utils	2df4e452cea39a9ec8fb543ce059f8d6	xdg-utils	baea6080dd821f5562d715887954c9d3	cmake	c7c9a62d6232cac66d4ea32d575c3e7c	llvm	203003e590307acca60eba586555388b	rocm	32cae3278ec0889923baf0e5a632e562
_md5_=279dd07e99b838bb464d3b4ed18b14ab
